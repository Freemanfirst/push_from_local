# 机器学习小课学习笔记

## 机器学习概念

### P1、机器学习简介（Introduction to Machine Learning）

- 机器学习技术的实际优势：

  1. 它是一个可以帮助你缩短编程时间的工具
  2. 借助机器学习，可以自定义自己的产品
  3. 借助机器学习，可以解决编程人员不知道的如何以人工方法解决的问题

- 机器学习技术背后的理念

  机器学习可以改变思考问题的方式。

  机器学习的关注点从数学科学转移到自然科学上，

  我们观察不确定的未知世界，开展实验，使用统计信息而非逻辑来分析实验结果。

### P2、框架处理、问题构建(Framing)

- （监督式）机器学习的简单定义：

  机器学习系统通过学习如何组合输入信息来对从未见过的数据做出有用的预测。

- 机器学习基本术语

  > | 分类模型                                             | 样本    **x** |
  > | ---------------------------------------------------- | ------------- |
  > | 特征   x<sub>1</sub>,x<sub>2</sub>,...,x<sub>N</sub> | 推断          |
  > | 标签  y                                              | 模型    y'    |
  > | 回归模型                                             | 训练          |

- 回归与分类

  **回归**模型可预测连续值。例如，回归模型做出的预测可回答如下问题：

  - 加利福尼亚州一栋房产的价值是多少？
  - 用户点击此广告的概率是多少？

  分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题：

  - 某个指定电子邮件是垃圾邮件还是非垃圾邮件？
  - 这是一张狗、猫还是仓鼠图片？

### P3、深入机器学习(Descending into ML)

- >  模型方程式：
  >
  >   y'  =  b  +  w<sub>1</sub>x<sub>1</sub>
  >
  > - y' 指的是预测标签（理想输出值）
  > - b指的是偏差（y轴截距）。而在一些机器学习文档中，它称为w<sub>0</sub>
  > - w<sub>1</sub>指的是特征 1 的权重。 权重与上文中用m表示的“斜率”的概念相同
  > - x<sub>1</sub>指的是特征（已知输入项）
  >
  > 下标（例如w<sub>1</sub>和x<sub>1</sub>）预示着可以用多个特征来表示更复杂的模型。例如，具有三个特征的模型可以采用一下方程式：
  >
  > y'  =  b  +  w<sub>1</sub>x<sub>1</sub>  +  w<sub>2</sub>x<sub>2</sub>  +   w<sub>3</sub>x<sub>3</sub>

- 简单来说，**训练**模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为**经验风险最小化**。

- 平方损失：一种常见的损失函数

  平方损失，又称L<sub>2</sub>损失。单个样本的平方损失如下：

   ```
   = the square of the difference between the label and the prediction
    = (observation - prediction(x))2
    = (y - y')2
   ```

- **均方误差** (**MSE**) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：

  ​								
  $$
  MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2
  $$
  虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。

### P4、降低损失(Reducing Loss)

- 迭代方法

  下图展示了机器学习算法用于训练模型的迭代试错过程：

  ![](/home/cmy/图片/GradientDescentDiagram.svg)

- 通常，不断的迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。

- 梯度下降法

  - 梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点的位置。

  - **超参数**是编程人员在机器学习算法中用于调整的旋钮。

  - 在梯度下降法中，**批量**指的是用于在单次迭代中计算梯度的样本总数。

  - **随机梯度下降法** (**SGD**) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。

  - **小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

### P5、使用TensorFlow的起始步骤

- 见实践教程

### P6、泛化(Generalization)

- 过拟合模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。

- 奥卡姆剃刀定律在机器学习方面的运用如下：

  > 机器学习模型越简单，良好的实证结果就越有可能不仅仅基于样本的特征。

- 机器学习模型旨在根据以前未见过的新数据做出良好预测。但是，如果您要根据数据集构建模型，如何获得以前未见过的数据呢？一种方法是将您的数据集分成两个子集：

  - **训练集** - 用于训练模型的子集。
  - **测试集** - 用于测试模型的子集。

  一般来说，在测试集上表现是否良好是衡量能否在新数据上表现良好的有用指标，前提是：

  - 测试集足够大。
  - 您不会反复使用相同的测试集来作假。

- **机器学习细则**

  以下三项基本假设阐明了泛化：

  - 我们从分布中随机抽取**独立同分布** (**i.i.d**) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。
  - 分布是**平稳的**；即分布在数据集内不会发生变化。
  - 我们从**同一分布**的数据划分中抽取样本。

  在实践中，我们有时会违背这些假设。例如：

  - 想象有一个选择要展示的广告的模型。如果该模型在某种程度上根据用户以前看过的广告选择广告，则会违背 i.i.d. 假设。
  - 想象有一个包含一年零售信息的数据集。用户的购买行为会出现季节性变化，这会违反平稳性。

  如果违背了上述三项基本假设中的任何一项，那么我们就必须密切注意指标。

- **总结**
  - 如果某个模型尝试紧密拟合训练数据，但却不能很好地泛化到新数据，就会发生过拟合。
  - 如果不符合监督式机器学习的关键假设，那么我们将失去对新数据进行预测这项能力的重要理论保证。

### P7、训练集和测试集(Training and Test Sets)

### P8、验证(Validation)

### P9、表示(Representation)

### P10、特征组合(Feature Crosses)

- **特征组合**是指两个或多个特征相乘形成的**合成特征**。特征的相乘组合可以提供超出这些特征单独能够提供的预测能力。

### P11、简单正则化(Regularization for Simplicity)

- **正则化**指的是降低模型的复杂度以减少过拟合。

- 过拟合(overfitting)指：出现只适用于训练集的的模型，尽管一次次迭代学习能将训练集的ｌｏｓｓ降到很低，但是在泛化到新数据、使用测试集测试的时候会让误差变大

- 

  



